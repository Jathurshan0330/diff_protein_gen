{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d244d992-03d7-41f5-b7d7-2b2958d40058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(os.environ[\"CUDA_VISIBLE_DEVICES\"],torch.cuda.device_count(), torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "245aa7f6-07e6-4e46-b932-69e6725f58d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0+cu102\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch import optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import time\n",
    "import math\n",
    "\n",
    "print(torch.__version__)\n",
    "torch.cuda.is_available()\n",
    "\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f039492-6add-4cb4-a4f4-cd9bea34fb8f",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ac07a5-190f-4539-a9af-8eced1362364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor \n",
    "mnist_train_dataset = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,                         \n",
    "    transform = transforms.Compose([ToTensor(), transforms.Resize(32)]),\n",
    "    download = True,            \n",
    ")\n",
    "mnist_test_dataset = datasets.MNIST(\n",
    "    root = 'data', \n",
    "    train = False, \n",
    "    transform = transforms.Compose([ToTensor(), transforms.Resize(32)]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3c42423-9902-4f36-9621-bc8c73106038",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(mnist_train_dataset, batch_size=32, shuffle=True)\n",
    "val_data_loader = torch.utils.data.DataLoader(mnist_test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "068a31d5-14cb-451b-8d8a-cbb61d1224e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([32, 1, 32, 32])\n",
      "Labels batch shape: torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum :0.0\n",
      "Maximum :0.9950827360153198\n"
     ]
    }
   ],
   "source": [
    "digit_imgs,digit_labels   = next(iter(train_data_loader))\n",
    "\n",
    "print(f\"Image batch shape: {digit_imgs.size()}\")\n",
    "print(f\"Labels batch shape: {digit_labels.size()}\")\n",
    "ind = 0\n",
    "img = digit_imgs[ind].squeeze()\n",
    "# label = train_labels[0]\n",
    "plt.figure()\n",
    "plt.imshow(img)\n",
    "plt.title(\"Real Image\")\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "# print(f\"Label: {label}\")\n",
    "print(f\"Minimum :{img.min()}\")\n",
    "print(f\"Maximum :{img.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ed5348-e87b-45b8-831e-d11736079a53",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eb4e74b-bfff-40b5-a37d-b00261b33248",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_nc=1, ngf = 16):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        self.enc1 = self.enc_block(in_ch = input_nc, out_ch = ngf, kernel_size=4, stride=2, padding = 1,bias = True, innermost = False  )\n",
    "        self.enc2 = self.enc_block(in_ch = ngf, out_ch = ngf*2, kernel_size=4, stride=2, padding = 1,bias = True, innermost = False  )\n",
    "        self.enc3 = self.enc_block(in_ch = ngf*2, out_ch = ngf*4, kernel_size=4, stride=2, padding = 1,bias = True, innermost = False  )\n",
    "        self.enc4 = self.enc_block(in_ch = ngf*4, out_ch = ngf*8, kernel_size=4, stride=2, padding = 1,bias = True, innermost = False  )\n",
    "        self.enc5 = self.enc_block(in_ch = ngf*8, out_ch = ngf*16, kernel_size=4, stride=2, padding = 1,bias = True, innermost = False  )\n",
    "        \n",
    "        \n",
    "    def enc_block(self, in_ch, out_ch, kernel_size=4, stride=2, padding = 1,bias = True, innermost = False):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, kernel_size=kernel_size,stride=stride, padding=padding, bias=bias),\n",
    "                nn.ReLU()\n",
    "                )\n",
    "      \n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.enc1(x)\n",
    "        x = self.enc2(x)\n",
    "        x = self.enc3(x)\n",
    "        x = self.enc4(x)\n",
    "        x = self.enc5(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class CNN_Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, lat_chan=256,out_ch = 1, ngf = 16 ):\n",
    "        super(CNN_Decoder, self).__init__()\n",
    "        self.dec1 = self.conv_up_block(in_ch = lat_chan , out_ch = ngf*8, \n",
    "                                       kernel_size=4, stride=2, padding = 1,bias = True)\n",
    "        self.dec2 = self.conv_up_block(in_ch = ngf*8 , out_ch = ngf*4, \n",
    "                                       kernel_size=4, stride=2, padding = 1,bias = True)\n",
    "        self.dec3 = self.conv_up_block(in_ch = ngf*4 , out_ch = ngf*2, \n",
    "                                       kernel_size=4, stride=2, padding = 1,bias = True)\n",
    "        self.dec4 = self.conv_up_block(in_ch = ngf*2 , out_ch = ngf, \n",
    "                                       kernel_size=4, stride=2, padding = 1,bias = True)\n",
    "        self.dec5 = self.conv_up_block(in_ch = ngf , out_ch = 1, \n",
    "                                       kernel_size=4, stride=2, padding = 1,bias = True)\n",
    "        # self.final = nn.Linear(16,out_ch)\n",
    "        \n",
    "        \n",
    "    def conv_up_block(self,  in_ch, out_ch, kernel_size=4, stride=2, padding = 1,bias = True,outermost = False):\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d( in_ch, out_ch, kernel_size=kernel_size, stride=stride, padding = padding,bias = bias),\n",
    "                nn.ReLU()\n",
    "                )\n",
    "        \n",
    "    def forward(self,x): \n",
    "        x = self.dec1(x)\n",
    "        \n",
    "        x = self.dec2(x)\n",
    "        \n",
    "        \n",
    "        x = self.dec3(x)\n",
    "        \n",
    "        x = self.dec4(x)\n",
    "        x = self.dec5(x)\n",
    "        # x = self.final(x)\n",
    "        # x = torch.moveaxis(x,-1,1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9307203-44eb-40e7-bb5e-75b9d6123bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 32, 32]) torch.Size([32, 256, 1, 1]) torch.Size([32, 1, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "encoder = CNN_Encoder().to(device)\n",
    "decoder = CNN_Decoder().to(device)\n",
    "\n",
    "lat_c = encoder(digit_imgs.to(device))\n",
    "out = decoder(lat_c)\n",
    "print(digit_imgs.shape,lat_c.shape,out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3e57a01-452e-4732-8be6-a93b5862d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_wandb = False\n",
    "criterion = nn.L1Loss()\n",
    "lr  = 0.001   \n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "n_epochs = 100 \n",
    "opt =  torch.optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=lr, betas=(beta_1, beta_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd61d7dc-0235-4423-96aa-0d7d9eee096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_loss(x_hat,x):\n",
    "    criterion = nn.MSELoss()\n",
    "    return criterion(x_hat,x)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count if self.count != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41ad5f37-8df1-4275-b47d-140ac4cb66a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(encoder,decoder,opt,data_loader,is_wandb=False,verbose_freq = 500,is_verbose = False):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    train_losses = AverageMeter()\n",
    "    end = time.time()\n",
    "    \n",
    "    for batch_idx, (x,_ ) in enumerate(data_loader): \n",
    "        data_time.update(time.time() - end)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        c_hat = encoder(x.to(device))\n",
    "        x_hat = decoder(c_hat)\n",
    "        \n",
    "        loss = recon_loss(x_hat,x.to(device))\n",
    "        \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        train_losses.update(loss.data.item())\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if is_verbose:\n",
    "            if (batch_idx+1) % verbose_freq == 0:\n",
    "                msg = 'Epoch: [{0}/{3}][{1}/{2}]\\t' \\\n",
    "                      'Time {batch_time.val:.3f}s ({batch_time.avg:.3f}s)\\t' \\\n",
    "                      'Speed {speed:.1f} samples/s\\t' \\\n",
    "                      'Data {data_time.val:.3f}s ({data_time.avg:.3f}s)\\t' \\\n",
    "                      'Loss {train_loss.val:.5f} ({train_loss.avg:.5f})\\t'.format(\n",
    "                          epoch_idx+1, batch_idx,len(data_loader), n_epochs , batch_time=batch_time,\n",
    "                          speed=x.size(0)/batch_time.val,\n",
    "                          data_time=data_time, train_loss=train_losses)\n",
    "                print(msg)\n",
    "        \n",
    "        if is_wandb:\n",
    "            wandb.log({\"batch_loss\": loss.data.item()})\n",
    "    \n",
    "    \n",
    "    if is_wandb:\n",
    "            wandb.log({\"train_epoch_loss\": train_losses.avg})\n",
    "            wandb.log({\"training time/Iter\": batch_time.sum/len(data_loader)})\n",
    "    \n",
    "    \n",
    "    # print(f\"Evaluation   Epoch : {epoch_idx+1}  =====================>\")\n",
    "    if is_verbose:\n",
    "        print(f\"Training Epoch Loss: {train_losses.avg}\")\n",
    "    return train_losses.avg\n",
    "        \n",
    "        \n",
    "def validate(encoder,decoder,data_loader,is_wandb=False,is_verbose = False):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    val_losses = AverageMeter()\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x,_) in enumerate(data_loader): \n",
    "            c_hat = encoder(x.to(device))\n",
    "            x_hat = decoder(c_hat)\n",
    "\n",
    "            loss = recon_loss(x_hat,x.to(device)) \n",
    "            \n",
    "            val_losses.update(loss.data.item())\n",
    "    \n",
    "    if is_wandb:\n",
    "            wandb.log({\"val_epoch_loss\": val_losses.avg})\n",
    "    \n",
    "    \n",
    "    # print(f\"Evaluation   Epoch : {epoch_idx+1}  =====================>\")\n",
    "    if is_verbose:\n",
    "        print(f\"Val Epoch Loss: {val_losses.avg}\")\n",
    "    return val_losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5a52cfb-cbbb-4882-af95-fcb2beb23602",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_path = \"./model_checkpoints/autoencoder_MNIST/\"\n",
    "if not os.path.exists(exp_path):\n",
    "    os.mkdir(exp_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31c393e-c8f2-4d5b-b5cf-3bc1616bdaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ="
     ]
    }
   ],
   "source": [
    "val_best_loss = 10000000\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "if not os.path.exists(exp_path):\n",
    "    os.mkdir(exp_path)\n",
    "print(\"Training \", end='')\n",
    "for epoch_idx in range(n_epochs):\n",
    "    print('=', end='')\n",
    "    # print(f\"Training Epoch : [{epoch_idx+1}/{n_epochs}]===============================================================================\")\n",
    "    \n",
    "    train_loss = train(encoder,decoder,opt,train_data_loader)    \n",
    "    val_losses = validate(encoder,decoder,val_data_loader) \n",
    "\n",
    "    train_loss_list.append(val_losses)\n",
    "    val_loss_list.append(val_losses)\n",
    "    if (val_losses < val_best_loss): \n",
    "        val_best_loss = val_losses\n",
    "        # print(\"Saving Best Model =======================================>\")\n",
    "        torch.save(encoder, f'{exp_path}/encoder.pth.tar')\n",
    "        torch.save(decoder, f'{exp_path}/decoder.pth.tar')\n",
    "    \n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(train_loss_list)\n",
    "plt.title(\"Train Loss\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(val_loss_list)\n",
    "plt.title(\"Val Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4efe74-ef3f-4354-986d-af766268efad",
   "metadata": {},
   "source": [
    "## Latent Space Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1568024-8c87-45f0-b055-b35f96558419",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b329026b-a0a9-4ccf-9455-af95f6c6cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_path = f'{exp_path}/encoder.pth.tar'\n",
    "encoder = torch.load(enc_path).to(device)\n",
    "encoder.eval()\n",
    "print(sum([np.prod(p.size()) for p in encoder.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597db7e-9b07-4d6f-af87-31ef7e4355ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (x,label) in enumerate(val_data_loader): \n",
    "    c_hat = encoder(x.to(device))\n",
    "    if batch_idx==0:\n",
    "        features = c_hat\n",
    "        label_list = label\n",
    "    else:\n",
    "        features = torch.cat((features,c_hat),axis = 0)\n",
    "        label_list = torch.cat((label_list,label),axis = 0)\n",
    "print(features.shape,label_list.shape)\n",
    "\n",
    "features = features.detach().cpu().numpy()\n",
    "label_list = label_list.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b457fa5-4b24-4279-9d3b-b332770de89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(random_state = 42, n_components=2,verbose=0, perplexity=40, n_iter=300).fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ee60ae-1413-4ead-a10c-5d458f2b1a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(tsne[:, 0], tsne[:, 1], s= 5, c=label_list, cmap='Spectral')\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.colorbar(boundaries=np.arange(11)-0.5).set_ticks(np.arange(10))\n",
    "plt.title('Visualizing MNIST latent space through t-SNE', fontsize=24);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b233f8-2b17-4934-a30c-c0f68df90c91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
